# NLP

**TF-IDF 原理**

Term Frequency-Inverse Document Frequency，意为词频率-逆文档频率，它主要是为了表现一个词对于一个在集合或语料库中的文档的重要程度。一般用于文档的关键词提取，文本分类的特征选择。

主要思想：如果一个词在一篇文档中出现的频率高，并且在其他文档中很少出现，则认为该词具有很好的区分能力，适合用来该文档和其他文章区分开来。

它包含两项，TF：一个词在特定的文档中的频率(占比)决定了它对该文档的代表性，词频越高，说明它在区分该文档内容属性方面的能力越强。IDF：一个词在语料库中所有文档中的出现文档数的逆频率表现了该词作为该文档的代表词的权重，逆频率越大，表示其代表该文档的能力越强。两项的乘积联合反映了该词对于该文档的重要程度，具体公式如下，
$$
TFIDF_{i,j} = \frac{N_{ij}}{N_i*} \times \log \frac{D}{D_{j}+1}
$$
其中，$TFIDF_{i,j}$ 表示特征词 $t_j$ 在文档 $d_i$ 中所占的权重，$N_{ij}$ 表示的是特征词 $t_j$ 在文档 $d_i$ 中的出现次数，$N_{i*}$ 表示的是文档 $d_i$ 中的所有特征词的出现次数，$D$ 表示语料库中所有文档数，$D_j$ 表示语料库中有特征词 $t_j$ 出现的文档数。

**☆ 为什么 TF-IDF 的形式是 log 的形式？**

> [TF-IDF模型的概率解释](https://coolshell.cn/articles/8422.html)

“的”，“什么”，“我们”这类高频词不会成为文档主题和搜索关键词的原因是它们不能提供**足够的信息**，而“iPhone 5”，“发布会”这样的词汇则信息量丰富。所谓信息是指对不确定性（熵）的减小程度，信息的单位是比特(bit)，信息量越大对于不确定性的减小程度越大。

比如，“2012美国大选”这个查询串包含了“2012”，“美国”和“大选”3个关键词，根据信息的定义，词的信息量等于它对不确定性的缩小程度。如果文档总数为2^30，其中2^14篇文档出现了“美国”，那么“美国”这个词就把文档的不确定性从2^30缩小为2^14，它所包含的信息量为log(2^30/2^14)=16；而只有2^10篇文档出现了“大选”，那么大选的信息量就是log(2^30/2^10)=20，比“美国”多了4个bit。而“的”，“什么”，“我们”这些高频词对减小文档不确定性几乎没有帮助，因而信息量为0。所以在 IDF 项上取 log 是为了表示词所包含的信息量。

Seq2seq结构：Encoder+Attention+Decoder

Word2vec，fasttext，ELMo，GPT，BERT的区别

